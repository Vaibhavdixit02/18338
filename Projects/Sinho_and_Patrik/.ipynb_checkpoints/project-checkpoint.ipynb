{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In this notebook we implement the 'Diagonal Thresholding' algorithm for sparse PCA introduced in https://www.tandfonline.com/doi/pdf/10.1198/jasa.2009.0121 . \n",
    "(maybe look at http://www.mit.edu/~yash/PAPERS/sparse.pdf ?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Plots, LinearAlgebra, Random, Distributions, Statistics, MultivariateStats, StatsBase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes in row-vector θ and outputs n samples from the Gaussian spiked covariance model\n",
    "function spiked(θ, n)\n",
    "#     θ = [sqrt(s)^(-1) for _ in 1:s]\n",
    "#     θ = vcat(θ, zeros(d - s))'\n",
    "    θ .* rand(Normal(), n) + rand(Normal(), n, size(θ)[2])\n",
    "end\n",
    "\n",
    "function embed(I, v)\n",
    "    arr = zeros(length(I), size(v)[2])\n",
    "    counter = 1\n",
    "    for index = 1:length(I)\n",
    "        if I[index]\n",
    "            arr[index, :] = v[counter, :]\n",
    "            counter += 1\n",
    "        end\n",
    "    end\n",
    "    return arr\n",
    "end\n",
    "\n",
    "function normalize_columns(X)\n",
    "    m = size(X)[2]\n",
    "    for col = 1:m\n",
    "        column = X[:, col]\n",
    "        n = norm(column)\n",
    "        if n != 0\n",
    "            X[:, col] = column .* n^(-1)\n",
    "        end\n",
    "    end\n",
    "    return X\n",
    "end\n",
    "\n",
    "# Number of samples\n",
    "n = 1000\n",
    "# Number of dimensions\n",
    "d = 1000\n",
    "# Sparsity of the principal eigenvector\n",
    "s = 10\n",
    "\n",
    "# Constructing the principal eigenvector\n",
    "θ = [sqrt(s)^(-1) for _ in 1:s]\n",
    "θ = vcat(θ, zeros(d - s))'\n",
    "# Generating a sample from the spiked covariance ensemble\n",
    "X = spiked(θ, n);\n",
    "\n",
    "# Step 2.: finding the top variances and storing in I\n",
    "vars = var(X, dims=1)\n",
    "α = 9\n",
    "σ² = median(vars)\n",
    "# ρ² = max(sum(vars .- σ²), σ² √(n\\d))\n",
    "# τ = (√n * ρ²)\\ √(σ² * (ρ² + σ²))\n",
    "cutoff = α * √(n\\log(n))\n",
    "I = vars .> σ² * (1 + cutoff)\n",
    "k = sum(I)\n",
    "\n",
    "# Step 3.: performing PCA on subset I of matrix\n",
    "Y = X[:, I[:]]\n",
    "Σ = cov(Y)\n",
    "# ef = eigen(Symmetric(Σ), k:k)\n",
    "ef = eigen(Symmetric(Σ))\n",
    "v = ef.vectors\n",
    "\n",
    "# Step 4.: thresholding the resulting eigenvectors\n",
    "τ = [mad(v[column, :], normalize = false) for column = 1:size(v)[2]].*(0.6745^(-1))\n",
    "v = v .* (broadcast(abs, v) .> (1* τ * √(2 * log(k))))\n",
    "v = normalize_columns(v)\n",
    "\n",
    "# Step 5.: returning to original dimension\n",
    "u = embed(I, v);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.2.0",
   "language": "julia",
   "name": "julia-1.2"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
